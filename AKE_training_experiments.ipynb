{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AKE_training_experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_LjP21Aauoo6",
        "M7JHfKV0x734"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eOuiHMXuoSe"
      },
      "source": [
        "# AUTOMATIC KEYWORD EXTRACTOR - Training experiments\n",
        "*Germán García García - gggsman@gmail.com*\n",
        "\n",
        "_________________________________\n",
        "\n",
        "MSC in Artificial Intelligence\n",
        "\n",
        "Final Master´s project: Applying Deep Learning Techniques to Terminology Extraction in Specific Domains\n",
        "\n",
        "Universidad Politecnica de Madrid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mBigCF53dHs"
      },
      "source": [
        "## Manual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfIv-KZUzy24"
      },
      "source": [
        "**Description:**\n",
        "\n",
        "The following Jupyter Notebook contains the cells of code needed to replicate the experiments presented in the document of the Final Master's Project, the main document can be found in the https://github.com/3Gsman/DeepTerminologyExtraction repository. It is an automatic keyword extractor system for training a model able to pick keywords from given texts. Acknowledgements to Sahrawat et ali."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_lReIqM0eqy"
      },
      "source": [
        "**Abstract:**\n",
        "\n",
        "Automatic terminology extraction or automatic keyphrase extraction is a very useful subfield of natural language processing when it comes to synthesizing information from texts in concise terms. In this master's thesis, this problem is approached with a sequence labeling approach using supervised deep learning techniques in specific domains, using bidirectional LTSM (Long short-term memory) neural networks and contextual word embeddings. A statistical significance study has been carried out to verify that the results presented in this work are significant. The final result in the F1 score in the Inspec dataset is 0.5730 slightly better than the higher result of the state of the art and it offers less dispersed results. Additionally, in this work the variation of samples in the training set is analyzed, a program to convert the datasets to a sequence labeling format needed for the final system is provided and there is an available sample program to test the keyphrase extractor in texts given by the user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDJMBk8D0egd"
      },
      "source": [
        "**Usage:**\n",
        "\n",
        "To use this notebook you need to have a compatible dataset. Within this notebook, in the [github repository](https://github.com/3Gsman/DeepTerminologyExtraction) of this work, a folder named *datasets* is available, with all the datasets used during the experimentarion. Other option is to format your own dataset, in the same github repository can be found a script able to convert from 20 available keyword extraction datasets to the format needed in this notebook, please follow the instructions found in the folder *format_dataset*. The last option is to create or adapt your own dataset, the format needed is similar to CoNLL-03 but with only 3 tags: *B-KEY* for the word that begins the keyword, *I-KEY* for the subsequent words of the keyword, and *O* if that word it is not a keyword. If you have any question, plese contact with the author, Germán García in gggsman@gmail.com.\n",
        "\n",
        "Once you have the dataset downloaded, upload it to google colab, in the left part of the interface a folder shaped button should be clicked, click and drag the dataset to the *Files* section and wait until is uploaded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoTeO6WihdMl"
      },
      "source": [
        "References:\n",
        "\n",
        "D. Sahrawat, D. Mahata, H. Zhang, M. Kulkarni, A. Sharma, R. Gosangi, A.Stent, Y. Kumar, R. R. Shah y R. Zimmermann, “Keyphrase Extraction as Se-quence Labeling Using Contextualized Embeddings”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LjP21Aauoo6"
      },
      "source": [
        "## Editable variables and hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RM_IpkHv_Ov"
      },
      "source": [
        "# Set download_model to True if you want to download the model at the end of the execution\n",
        "download_model = False"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_BY6vtrxkEd"
      },
      "source": [
        "# Edit the hyperparameters if you want to change the traning behaviour\n",
        "class hyperparam:\n",
        "  embedding = 'Bert'\n",
        "  embedding_path = ''\n",
        "  dataset_base_path = ''\n",
        "  dataset = 'dataset'\n",
        "  output_base_path = 'result/'\n",
        "  iteration = ''\n",
        "  gpu = 1\n",
        "  lr = 0.05\n",
        "  anneal_factor = 0.5\n",
        "  patience = 4\n",
        "  batch_size = 4\n",
        "  num_epochs = 10\n",
        "  threads = 12\n",
        "  param_selection_mode = False\n",
        "  use_tensorboard = False\n",
        "  no_dev = False\n",
        "  use_crf = True\n",
        "  rnn_layers = 3\n",
        "  hidden_size = 128\n",
        "  dropout = 0.3\n",
        "  word_dropout = 0.05\n",
        "  locked_dropout = 0.5\n",
        "  not_in_memory = False"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jarXR_dAx4NQ"
      },
      "source": [
        "## System startup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBXenH5L4kLx"
      },
      "source": [
        "This section unzips the dataset and download the needed libraries to make this notebook work, also it imports the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUjzmKParSgA",
        "outputId": "6c9a77b4-b585-471d-a49f-7b527afcace9"
      },
      "source": [
        "!unzip dataset.zip"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  dataset.zip\n",
            "   creating: dataset/\n",
            "  inflating: dataset/dev.txt         \n",
            "  inflating: dataset/test.txt        \n",
            "  inflating: dataset/train.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6B6diatIetE",
        "outputId": "e0aca218-0eee-43e6-dfab-c32c277e5b02"
      },
      "source": [
        "!pip install flair\n",
        "!pip install pytorch_transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flair\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/3a/1b46a0220d6176b22bcb9336619d1731301bc2c75fa926a9ef953e6e4d58/flair-0.8.0.post1-py3-none-any.whl (284kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 13.1MB/s \n",
            "\u001b[?25hCollecting torch<=1.7.1,>=1.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8MB 23kB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading https://files.pythonhosted.org/packages/35/03/071adc023c0a7e540cf4652fa9cad13ab32e6ae469bf0cc0262045244812/huggingface_hub-0.0.13-py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (0.22.2.post1)\n",
            "Requirement already satisfied: gensim<=3.8.3,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
            "Collecting janome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7MB 49.9MB/s \n",
            "\u001b[?25hCollecting transformers>=4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 28.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.41.1)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/fb/73/994edfcba74443146c84b91921fcc269374354118d4f452fb0c54c1cbb12/Deprecated-1.2.12-py2.py3-none-any.whl\n",
            "Collecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 43.4MB/s \n",
            "\u001b[?25hCollecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/2d/b1d99e9ad157dd7de9cd0d36a8a5876b13b55e4b75f7498bc96035fb4e96/sqlitedict-1.7.0.tar.gz\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n",
            "Requirement already satisfied: numpy<1.20.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.19.5)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/72/a3add0e4eec4eb9e2569554f7c70f4a3c27712f40e3284d483e88094cc0e/langdetect-1.0.9.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 40.6MB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/da/d215a091986e5f01b80f5145cff6f22e2dc57c6b048aab2e882a07018473/ftfy-6.0.3.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.4MB/s \n",
            "\u001b[?25hCollecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/71/70/48a0bd55f79c328504fe6fe7ae8ff651f77a2aadbb1911701385d9bb5ca3/konoha-4.6.5-py3-none-any.whl\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.2)\n",
            "Collecting bpemb>=0.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/6f/9191b85109772636a8f8accb122900c34db26c091d2793218aa94954524c/bpemb-0.3.3-py3-none-any.whl\n",
            "Collecting sentencepiece==0.1.95\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 31.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.1)\n",
            "Collecting gdown==3.12.2\n",
            "  Downloading https://files.pythonhosted.org/packages/50/21/92c3cfe56f5c0647145c4b0083d0733dd4890a057eb100a8eeddf949ffe9/gdown-3.12.2.tar.gz\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<=1.7.1,>=1.5.0->flair) (3.7.4.3)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (4.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (2.23.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (5.1.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (1.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (3.13)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 40.6MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 32.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (2.5.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (3.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->huggingface-hub->flair) (3.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->flair) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->flair) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->flair) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->flair) (2021.5.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-cp37-none-any.whl size=9705 sha256=3c226f7e7cd075df166c3d247c3931c34c989ba0ff1195ae0299629c11061e2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/d0/d7/d9983facc6f2775411803e0e2d30ebf98efbf2fc6e57701e09\n",
            "Successfully built gdown\n",
            "Building wheels for collected packages: mpld3, sqlitedict, langdetect, ftfy, segtok, overrides\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp37-none-any.whl size=116704 sha256=63494ded0b0d27ec9bf76ec5d9a9b5451ae80dd72a47c7e16dafb70f867a00b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-cp37-none-any.whl size=14393 sha256=cfec8e29cd6c486c17515bfaa0a8645d8dab1b54104a48ebce189ee4047bd017\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/c6/4f/2c64a43f041415eb8b8740bd80e15e92f0d46c5e464d8e4b9b\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-cp37-none-any.whl size=993242 sha256=7ad2a7cbb7dcc06bcb73d3c57b5db10cb2c19995c0e5a3618e9232f1572b1766\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/18/13/038c34057808931c7ddc6c92d3aa015cf1a498df5a70268996\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-cp37-none-any.whl size=41935 sha256=07a761dfe0db10d0d107564a561b217c5b95fef050b0159b595f3bdce530e483\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/2c/e6/109c8a28fef7a443f67ba58df21fe1d0067ac3322e75e6b0b7\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp37-none-any.whl size=25031 sha256=9ec2c4a2ef057e7c4f48b8d5d248a5e9e3abd3eba7bc121194e164ac69657447\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-cp37-none-any.whl size=10187 sha256=9db50166d56af2540e60acd8d99a9482e82fd28fbb165aba53a91a2304edac0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
            "Successfully built mpld3 sqlitedict langdetect ftfy segtok overrides\n",
            "\u001b[31mERROR: torchvision 0.10.0+cu102 has requirement torch==1.9.0, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: transformers 4.8.2 has requirement huggingface-hub==0.0.12, but you'll have huggingface-hub 0.0.13 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: konoha 4.6.5 has requirement importlib-metadata<4.0.0,>=3.7.0, but you'll have importlib-metadata 4.6.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: konoha 4.6.5 has requirement requests<3.0.0,>=2.25.1, but you'll have requests 2.23.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, huggingface-hub, janome, sacremoses, tokenizers, transformers, deprecated, mpld3, sqlitedict, langdetect, ftfy, segtok, overrides, konoha, sentencepiece, bpemb, gdown, flair\n",
            "  Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Found existing installation: gdown 3.6.4\n",
            "    Uninstalling gdown-3.6.4:\n",
            "      Successfully uninstalled gdown-3.6.4\n",
            "Successfully installed bpemb-0.3.3 deprecated-1.2.12 flair-0.8.0.post1 ftfy-6.0.3 gdown-3.12.2 huggingface-hub-0.0.13 janome-0.4.1 konoha-4.6.5 langdetect-1.0.9 mpld3-0.3 overrides-3.1.0 sacremoses-0.0.45 segtok-1.5.10 sentencepiece-0.1.95 sqlitedict-1.7.0 tokenizers-0.10.3 torch-1.7.1 transformers-4.8.2\n",
            "Collecting pytorch_transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 11.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (0.0.45)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (0.1.95)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/65/c5fa632ca1c243d0f757c09cc4656f361dc1a9555f0b6d442e374f7ca8f6/boto3-1.17.109-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 21.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (1.7.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (1.19.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (1.24.3)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.21.0,>=1.20.109\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/53/779ef784d896647f53e629143b5002adee52ea4574aa3dfaa8a9fe322c92/botocore-1.20.109-py2.py3-none-any.whl (7.7MB)\n",
            "\u001b[K     |████████████████████████████████| 7.7MB 21.5MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.109->boto3->pytorch_transformers) (2.8.1)\n",
            "\u001b[31mERROR: botocore 1.20.109 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-transformers\n",
            "Successfully installed boto3-1.17.109 botocore-1.20.109 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6bv6_26uj6r"
      },
      "source": [
        "import sys\n",
        "from typing import List\n",
        "import argparse\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, FlairEmbeddings, CharacterEmbeddings, BertEmbeddings, TransformerXLEmbeddings, ELMoTransformerEmbeddings, ELMoEmbeddings,OpenAIGPTEmbeddings, RoBERTaEmbeddings,XLMEmbeddings, XLNetEmbeddings, OpenAIGPT2Embeddings\n",
        "from flair.datasets import DataLoader\n",
        "from flair.data import Corpus, Sentence\n",
        "from flair.datasets import ColumnCorpus\n",
        "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
        "import flair.datasets\n",
        "from flair.visual.training_curves import Plotter\n",
        "from torch.utils.data import DataLoader\n",
        "from flair.models import SequenceTagger\n",
        "from google.colab import files\n",
        "from flair.trainers import ModelTrainer\n",
        "import torch.optim as optim\n",
        "from pytorch_transformers import BertTokenizer\n",
        "from flair.models import SequenceTagger\n",
        "from pytorch_transformers import BertTokenizer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7JHfKV0x734"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEVGHJtUx8eo"
      },
      "source": [
        "This section contains the functions and the call of the training function that gets the hyperparameters set above and trains a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyjIl55QwWaZ"
      },
      "source": [
        "def bs(tokenizer,x,l,r,max_seq_len):\n",
        "    if r>=l:\n",
        "        mid = int(l + (r - l)/2)\n",
        "        res=verifymid(tokenizer,x,mid,max_seq_len)\n",
        "        if res==3:\n",
        "            return mid\n",
        "        elif res==2:\n",
        "            return bs(tokenizer,x,mid+1,r,max_seq_len)\n",
        "        else:\n",
        "            return bs(tokenizer,x,l,mid-1,max_seq_len)\n",
        "            \n",
        "    else:\n",
        "        print(\"wrong binary search\")\n",
        "        sys.exit()\n",
        "\n",
        "\n",
        "def verifymid(tokenizer,x,mid,max_seq_len):\n",
        "    limit=mid\n",
        "    lw=x.to_tokenized_string().split(\" \")\n",
        "    lw=lw[:limit]\n",
        "    sent=\" \".join(lw)\n",
        "    tokenized_text = tokenizer.tokenize(sent)\n",
        "    if len(tokenized_text)>max_seq_len:\n",
        "        return 1\n",
        "    else:\n",
        "        if verifymid_1(tokenizer,x,mid+1,max_seq_len)==True:\n",
        "            return 2\n",
        "        return 3\n",
        "        \n",
        "        \n",
        "def verifymid_1(tokenizer,x,mid,max_seq_len):\n",
        "    limit=mid\n",
        "    lw=x.to_tokenized_string().split(\" \")\n",
        "    lw=lw[:limit]\n",
        "    sent=\" \".join(lw)\n",
        "    tokenized_text = tokenizer.tokenize(sent)\n",
        "    if len(tokenized_text)>max_seq_len:\n",
        "        return False\n",
        "    else:\n",
        "        return True"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bJHMtlcIbEP"
      },
      "source": [
        "def train(data_path, list_embedding, output, hyperparameter ):\n",
        "\n",
        "    # define columns\n",
        "    columns = {0: 'text', 1: 'ner'}\n",
        "\n",
        "    # retrieve corpus using column format, data folder and the names of the train, dev and test files\n",
        "    if hyperparam.no_dev==True:\n",
        "        corpus: Corpus = ColumnCorpus(data_path, \n",
        "                                      columns, \n",
        "                                      train_file='train.txt',\n",
        "                                      test_file='test.txt',\n",
        "                                      in_memory=not hyperparam.not_in_memory\n",
        "                                     )\n",
        "        \n",
        "    else:\n",
        "        corpus: Corpus = ColumnCorpus(data_path,\n",
        "                                      columns,\n",
        "                                      train_file='train.txt',\n",
        "                                      test_file='test.txt',\n",
        "                                      dev_file='dev.txt',\n",
        "                                      in_memory=not hyperparam.not_in_memory\n",
        "                                      )\n",
        "\n",
        "    # 2. what tag do we want to predict?\n",
        "    tag_type = 'ner'\n",
        "\n",
        "    # 3. make the tag dictionary from the corpus\n",
        "    tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
        "    print(tag_dictionary.idx2item)\n",
        "    \n",
        "    stats=corpus.obtain_statistics()\n",
        "    print(\"Original\\n\",stats)\n",
        "    \n",
        "    # BERT Embeddings\n",
        "    if hyperparam.embedding==\"Bert\":\n",
        "\n",
        "        print(\"Tokenizer\",hyperparam.embedding)\n",
        "        if hyperparam.embedding_path!=\"\":\n",
        "            tokenizer = BertTokenizer.from_pretrained(hyperparam.embedding_path)\n",
        "        else:\n",
        "            tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "        \n",
        "        max_seq_len=500   \n",
        "        print(\"taking max seq len as \",max_seq_len)        \n",
        "\n",
        "        new_train=[]\n",
        "        for x in corpus.train:\n",
        "\n",
        "            tokenized_text = tokenizer.tokenize(x.to_plain_string())\n",
        "\n",
        "            if len(tokenized_text)<=max_seq_len:\n",
        "                new_train.append(x)\n",
        "            \n",
        "            else:          \n",
        "                limit=bs(tokenizer,x,1,max_seq_len,max_seq_len)\n",
        "                lw=x.to_tokenized_string().split(\" \")\n",
        "                lw=lw[:limit]\n",
        "                sent=\" \".join(lw)\n",
        "                tokenized_text = tokenizer.tokenize(sent)\n",
        "                \n",
        "                if len(tokenized_text)>max_seq_len:\n",
        "                    print(\"wrong binary search 1\")\n",
        "                    sys.exit()\n",
        "\n",
        "                new_sent=Sentence(sent)\n",
        "                for index in range(len(new_sent)):\n",
        "                    try:\n",
        "                        new_sent[index].add_tag('ner', x[index].get_tag('ner').value)\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                new_train.append(new_sent)\n",
        "\n",
        "        new_test=[]\n",
        "        for x in corpus.test:\n",
        "\n",
        "            tokenized_text = tokenizer.tokenize(x.to_plain_string())\n",
        "\n",
        "            if len(tokenized_text)<=max_seq_len:\n",
        "                new_test.append(x)\n",
        "            \n",
        "            else:           \n",
        "                limit=bs(tokenizer,x,1,max_seq_len,max_seq_len)\n",
        "                lw=x.to_tokenized_string().split(\" \")\n",
        "                lw=lw[:limit]\n",
        "                sent=\" \".join(lw)\n",
        "                tokenized_text = tokenizer.tokenize(sent)\n",
        "\n",
        "                if len(tokenized_text)>max_seq_len:\n",
        "                    print(\"wrong binary search 1\")\n",
        "                    sys.exit()\n",
        "\n",
        "                new_sent=Sentence(sent)\n",
        "                for index in range(len(new_sent)):\n",
        "                    try:\n",
        "                        new_sent[index].add_tag('ner', x[index].get_tag('ner').value)\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                new_test.append(new_sent)\n",
        "\n",
        "        new_dev=[]\n",
        "        for x in corpus.dev:\n",
        "\n",
        "            tokenized_text = tokenizer.tokenize(x.to_plain_string())\n",
        "\n",
        "            if len(tokenized_text)<=max_seq_len:\n",
        "                new_dev.append(x)\n",
        "            \n",
        "            else:           \n",
        "                limit=bs(tokenizer,x,1,max_seq_len,max_seq_len)\n",
        "                lw=x.to_tokenized_string().split(\" \")\n",
        "                lw=lw[:limit]\n",
        "                sent=\" \".join(lw)\n",
        "                tokenized_text = tokenizer.tokenize(sent)\n",
        "\n",
        "                if len(tokenized_text)>max_seq_len:\n",
        "                    print(\"wrong binary search 1\")\n",
        "                    sys.exit()\n",
        "\n",
        "                new_sent=Sentence(sent)\n",
        "                for index in range(len(new_sent)):\n",
        "                    try:\n",
        "                        new_sent[index].add_tag('ner', x[index].get_tag('ner').value)\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                new_dev.append(new_sent)   \n",
        "\n",
        "        corpus._train=new_train\n",
        "        corpus._test=new_test\n",
        "        corpus._dev=new_dev\n",
        "        stats=corpus.obtain_statistics()\n",
        "        print(\"Modified\",stats)  \n",
        "\n",
        "    # RoBERTa Embeddings\n",
        "    elif hyperparam.embedding==\"RoBERTa\":\n",
        "\n",
        "        print(\"Tokenizer\",hyperparam.embedding)\n",
        "\n",
        "        print(\"Using Bert tokenizer bert-base-uncased\")\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        \n",
        "        max_seq_len=500   \n",
        "        print(\"taking max seq len as \",max_seq_len)\n",
        "           \n",
        "        new_train=[]\n",
        "        for x in corpus.train:\n",
        "\n",
        "            tokenized_text = tokenizer.tokenize(x.to_plain_string())\n",
        "\n",
        "            if len(tokenized_text)<=max_seq_len:\n",
        "                new_train.append(x)\n",
        "            \n",
        "            else:            \n",
        "                limit=bs(tokenizer,x,1,max_seq_len,max_seq_len)\n",
        "                lw=x.to_tokenized_string().split(\" \")\n",
        "                lw=lw[:limit]\n",
        "                sent=\" \".join(lw)\n",
        "                tokenized_text = tokenizer.tokenize(sent)\n",
        "\n",
        "                if len(tokenized_text)>max_seq_len:\n",
        "                    print(\"wrong binary search 1\")\n",
        "                    sys.exit()\n",
        "\n",
        "                new_sent=Sentence(sent)\n",
        "                for index in range(len(new_sent)):\n",
        "                    try:\n",
        "                        new_sent[index].add_tag('ner', x[index].get_tag('ner').value)\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                new_train.append(new_sent)\n",
        "\n",
        "        new_test=[]\n",
        "        for x in corpus.test:\n",
        "            tokenized_text = tokenizer.tokenize(x.to_plain_string())\n",
        "            if len(tokenized_text)<=max_seq_len:\n",
        "                new_test.append(x)\n",
        "            \n",
        "            else:            \n",
        "                limit=bs(tokenizer,x,1,max_seq_len,max_seq_len)\n",
        "                lw=x.to_tokenized_string().split(\" \")\n",
        "                lw=lw[:limit]\n",
        "                sent=\" \".join(lw)\n",
        "                tokenized_text = tokenizer.tokenize(sent)\n",
        "\n",
        "                if len(tokenized_text)>max_seq_len:\n",
        "                    print(\"wrong binary search 1\")\n",
        "                    sys.exit()\n",
        "\n",
        "                new_sent=Sentence(sent)\n",
        "                for index in range(len(new_sent)):\n",
        "                    try:\n",
        "                        new_sent[index].add_tag('ner', x[index].get_tag('ner').value)\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                new_test.append(new_sent)\n",
        "\n",
        "        new_dev=[]\n",
        "        for x in corpus.dev:\n",
        "\n",
        "            tokenized_text = tokenizer.tokenize(x.to_plain_string())\n",
        "            \n",
        "            if len(tokenized_text)<=max_seq_len:\n",
        "                new_dev.append(x)\n",
        "            \n",
        "            else:            \n",
        "                limit=bs(tokenizer,x,1,max_seq_len,max_seq_len)\n",
        "                lw=x.to_tokenized_string().split(\" \")\n",
        "                lw=lw[:limit]\n",
        "                sent=\" \".join(lw)\n",
        "                tokenized_text = tokenizer.tokenize(sent)\n",
        "\n",
        "                if len(tokenized_text)>max_seq_len:\n",
        "                    print(\"wrong binary search 1\")\n",
        "                    sys.exit()\n",
        "\n",
        "                new_sent=Sentence(sent)\n",
        "                for index in range(len(new_sent)):\n",
        "                    new_sent[index].add_tag('ner', x[index].get_tag('ner').value)\n",
        "\n",
        "                new_dev.append(new_sent)             \n",
        "        \n",
        "        corpus._train=new_train\n",
        "        corpus._test=new_test\n",
        "        corpus._dev=new_dev\n",
        "        stats=corpus.obtain_statistics()\n",
        "        print(\"Modified\",stats)  \n",
        "\n",
        "\n",
        "    # 4. initialize embeddings\n",
        "    embedding_types: List[TokenEmbeddings] = list_embedding\n",
        "\n",
        "    embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "    # 5. initialize sequence tagger\n",
        "\n",
        "    tagger: SequenceTagger = SequenceTagger(hidden_size=hyperparam.hidden_size,\n",
        "                                        embeddings=embeddings,\n",
        "                                        tag_dictionary=tag_dictionary,\n",
        "                                        tag_type=tag_type,\n",
        "                                        use_crf=hyperparam.use_crf,\n",
        "                                        rnn_layers=hyperparam.rnn_layers,\n",
        "                                        dropout=hyperparam.dropout, word_dropout=hyperparam.word_dropout, locked_dropout=hyperparam.locked_dropout\n",
        "                                        )\n",
        "\n",
        "    # 6. initialize trainer\n",
        "\n",
        "    trainer: ModelTrainer = ModelTrainer(tagger, \n",
        "                                        corpus,\n",
        "                                        use_tensorboard=hyperparam.use_tensorboard,\n",
        "                                        optimizer=optim.SGD\n",
        "                                        ) \n",
        "\n",
        "    # 7. start training\n",
        "    trainer.train(output,\n",
        "              learning_rate=hyperparam.lr,\n",
        "              mini_batch_size=hyperparam.batch_size,\n",
        "              anneal_factor=hyperparam.anneal_factor,\n",
        "              patience=hyperparam.patience,\n",
        "              max_epochs=hyperparam.num_epochs,\n",
        "              param_selection_mode=hyperparam.param_selection_mode,\n",
        "              num_workers=hyperparam.threads,\n",
        "              \n",
        "              )\n",
        "    return trainer\n",
        "    "
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4beFhlmIpwx",
        "outputId": "e3933ddd-34fe-4e2b-ee67-d85ca988ba1d"
      },
      "source": [
        "if hyperparam.embedding=='Bert':\n",
        "    if hyperparam.embedding_path!=\"\":\n",
        "        embedding=BertEmbeddings(hyperparam.embedding_path)\n",
        "    else:\n",
        "        embedding=BertEmbeddings()\n",
        "\n",
        "if hyperparam.embedding=='ELMo':\n",
        "    if hyperparam.embedding_path!=\"\":\n",
        "        embedding=ELMoEmbeddings(hyperparam.embedding_path)\n",
        "    else:\n",
        "        embedding=ELMoEmbeddings()\n",
        "\n",
        "if hyperparam.embedding=='RoBERTa':\n",
        "    if hyperparam.embedding_path!=\"\":\n",
        "        embedding=RoBERTaEmbeddings(hyperparam.embedding_path)\n",
        "    else:\n",
        "        embedding=RoBERTaEmbeddings()\n",
        "\n",
        "output=hyperparam.output_base_path+hyperparam.embedding+\"_\"+hyperparam.embedding_path +\"_\"+hyperparam.dataset+hyperparam.iteration+\"_bs_\"+str(hyperparam.batch_size)+ \"_lr_\"+str(hyperparam.lr)+ '_af_'+str(hyperparam.anneal_factor)+ '_p_'+ str(hyperparam.patience) +\\\n",
        "               \"_hsize_\"+str(hyperparam.hidden_size)+\"_crf_\"+str(int(hyperparam.use_crf))+\"_lrnn_\"+str(hyperparam.rnn_layers)+\"_dp_\"+str(hyperparam.dropout)+\"_wdp_\"+str(hyperparam.word_dropout)+\"_ldp_\"+str(hyperparam.locked_dropout)+\"/\"\n",
        "dataset_path=hyperparam.dataset_base_path+hyperparam.dataset+\"/\"\n",
        "\n",
        "print(output)\n",
        "print(dataset_path)\n",
        "\n",
        "print(\"\\nHyper-Parameters\\n\")\n",
        "arguments=vars(hyperparam)\n",
        "for i in arguments:\n",
        "    print('{0:25}  {1}'.format(i+\":\", str(arguments[i])))\n",
        "    print(i+\" : \"+str(arguments[i]))\n",
        "\n",
        "trainer=train(dataset_path,[embedding],output,hyperparam)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated method __init__. (Use 'TransformerWordEmbeddings' for all transformer-based word embeddings) -- Deprecated since version 0.4.5.\n",
            "  \"\"\"\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "result/Bert__dataset_bs_4_lr_0.05_af_0.5_p_4_hsize_128_crf_1_lrnn_3_dp_0.3_wdp_0.05_ldp_0.5/\n",
            "dataset/\n",
            "\n",
            "Hyper-Parameters\n",
            "\n",
            "__module__:                __main__\n",
            "__module__ : __main__\n",
            "embedding:                 Bert\n",
            "embedding : Bert\n",
            "embedding_path:            \n",
            "embedding_path : \n",
            "dataset_base_path:         \n",
            "dataset_base_path : \n",
            "dataset:                   dataset\n",
            "dataset : dataset\n",
            "output_base_path:          result/\n",
            "output_base_path : result/\n",
            "iteration:                 \n",
            "iteration : \n",
            "gpu:                       1\n",
            "gpu : 1\n",
            "lr:                        0.05\n",
            "lr : 0.05\n",
            "anneal_factor:             0.5\n",
            "anneal_factor : 0.5\n",
            "patience:                  4\n",
            "patience : 4\n",
            "batch_size:                4\n",
            "batch_size : 4\n",
            "num_epochs:                10\n",
            "num_epochs : 10\n",
            "threads:                   12\n",
            "threads : 12\n",
            "param_selection_mode:      False\n",
            "param_selection_mode : False\n",
            "use_tensorboard:           False\n",
            "use_tensorboard : False\n",
            "no_dev:                    False\n",
            "no_dev : False\n",
            "use_crf:                   True\n",
            "use_crf : True\n",
            "rnn_layers:                3\n",
            "rnn_layers : 3\n",
            "hidden_size:               128\n",
            "hidden_size : 128\n",
            "dropout:                   0.3\n",
            "dropout : 0.3\n",
            "word_dropout:              0.05\n",
            "word_dropout : 0.05\n",
            "locked_dropout:            0.5\n",
            "locked_dropout : 0.5\n",
            "not_in_memory:             False\n",
            "not_in_memory : False\n",
            "__dict__:                  <attribute '__dict__' of 'hyperparam' objects>\n",
            "__dict__ : <attribute '__dict__' of 'hyperparam' objects>\n",
            "__weakref__:               <attribute '__weakref__' of 'hyperparam' objects>\n",
            "__weakref__ : <attribute '__weakref__' of 'hyperparam' objects>\n",
            "__doc__:                   None\n",
            "__doc__ : None\n",
            "2021-07-12 12:44:42,307 Reading data from dataset\n",
            "2021-07-12 12:44:42,308 Train: dataset/train.txt\n",
            "2021-07-12 12:44:42,310 Dev: dataset/dev.txt\n",
            "2021-07-12 12:44:42,312 Test: dataset/test.txt\n",
            "[b'<unk>', b'O', b'B-KEY', b'I-KEY', b'<START>', b'<STOP>']\n",
            "Original\n",
            " {\n",
            "    \"TRAIN\": {\n",
            "        \"dataset\": \"TRAIN\",\n",
            "        \"total_number_of_documents\": 1000,\n",
            "        \"number_of_documents_per_class\": {},\n",
            "        \"number_of_tokens_per_tag\": {},\n",
            "        \"number_of_tokens\": {\n",
            "            \"total\": 141508,\n",
            "            \"min\": 15,\n",
            "            \"max\": 557,\n",
            "            \"avg\": 141.508\n",
            "        }\n",
            "    },\n",
            "    \"TEST\": {\n",
            "        \"dataset\": \"TEST\",\n",
            "        \"total_number_of_documents\": 500,\n",
            "        \"number_of_documents_per_class\": {},\n",
            "        \"number_of_tokens_per_tag\": {},\n",
            "        \"number_of_tokens\": {\n",
            "            \"total\": 67300,\n",
            "            \"min\": 23,\n",
            "            \"max\": 384,\n",
            "            \"avg\": 134.6\n",
            "        }\n",
            "    },\n",
            "    \"DEV\": {\n",
            "        \"dataset\": \"DEV\",\n",
            "        \"total_number_of_documents\": 500,\n",
            "        \"number_of_documents_per_class\": {},\n",
            "        \"number_of_tokens_per_tag\": {},\n",
            "        \"number_of_tokens\": {\n",
            "            \"total\": 66291,\n",
            "            \"min\": 16,\n",
            "            \"max\": 330,\n",
            "            \"avg\": 132.582\n",
            "        }\n",
            "    }\n",
            "}\n",
            "Tokenizer Bert\n",
            "taking max seq len as  500\n",
            "Modified {\n",
            "    \"TRAIN\": {\n",
            "        \"dataset\": \"TRAIN\",\n",
            "        \"total_number_of_documents\": 1000,\n",
            "        \"number_of_documents_per_class\": {},\n",
            "        \"number_of_tokens_per_tag\": {},\n",
            "        \"number_of_tokens\": {\n",
            "            \"total\": 141283,\n",
            "            \"min\": 15,\n",
            "            \"max\": 396,\n",
            "            \"avg\": 141.283\n",
            "        }\n",
            "    },\n",
            "    \"TEST\": {\n",
            "        \"dataset\": \"TEST\",\n",
            "        \"total_number_of_documents\": 500,\n",
            "        \"number_of_documents_per_class\": {},\n",
            "        \"number_of_tokens_per_tag\": {},\n",
            "        \"number_of_tokens\": {\n",
            "            \"total\": 67308,\n",
            "            \"min\": 23,\n",
            "            \"max\": 384,\n",
            "            \"avg\": 134.616\n",
            "        }\n",
            "    },\n",
            "    \"DEV\": {\n",
            "        \"dataset\": \"DEV\",\n",
            "        \"total_number_of_documents\": 500,\n",
            "        \"number_of_documents_per_class\": {},\n",
            "        \"number_of_tokens_per_tag\": {},\n",
            "        \"number_of_tokens\": {\n",
            "            \"total\": 66291,\n",
            "            \"min\": 16,\n",
            "            \"max\": 330,\n",
            "            \"avg\": 132.582\n",
            "        }\n",
            "    }\n",
            "}\n",
            "2021-07-12 12:44:56,228 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 12:44:56,234 Model: \"SequenceTagger(\n",
            "  (embeddings): StackedEmbeddings(\n",
            "    (list_embedding_0): BertEmbeddings(\n",
            "      (model): BertModel(\n",
            "        (embeddings): BertEmbeddings(\n",
            "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "          (position_embeddings): Embedding(512, 768)\n",
            "          (token_type_embeddings): Embedding(2, 768)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (encoder): BertEncoder(\n",
            "          (layer): ModuleList(\n",
            "            (0): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (1): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (2): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (3): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (4): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (5): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (6): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (7): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (8): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (9): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (10): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (11): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (pooler): BertPooler(\n",
            "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (activation): Tanh()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=3072, out_features=3072, bias=True)\n",
            "  (rnn): LSTM(3072, 128, num_layers=3, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  (linear): Linear(in_features=256, out_features=6, bias=True)\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2021-07-12 12:44:56,236 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 12:44:56,247 Corpus: \"Corpus: 1000 train + 500 dev + 500 test sentences\"\n",
            "2021-07-12 12:44:56,249 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 12:44:56,251 Parameters:\n",
            "2021-07-12 12:44:56,256  - learning_rate: \"0.05\"\n",
            "2021-07-12 12:44:56,259  - mini_batch_size: \"4\"\n",
            "2021-07-12 12:44:56,261  - patience: \"4\"\n",
            "2021-07-12 12:44:56,263  - anneal_factor: \"0.5\"\n",
            "2021-07-12 12:44:56,265  - max_epochs: \"10\"\n",
            "2021-07-12 12:44:56,270  - shuffle: \"True\"\n",
            "2021-07-12 12:44:56,273  - train_with_dev: \"False\"\n",
            "2021-07-12 12:44:56,275  - batch_growth_annealing: \"False\"\n",
            "2021-07-12 12:44:56,279 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 12:44:56,282 Model training base path: \"result/Bert__dataset_bs_4_lr_0.05_af_0.5_p_4_hsize_128_crf_1_lrnn_3_dp_0.3_wdp_0.05_ldp_0.5\"\n",
            "2021-07-12 12:44:56,285 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 12:44:56,288 Device: cuda:0\n",
            "2021-07-12 12:44:56,292 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 12:44:56,296 Embeddings storage mode: cpu\n",
            "2021-07-12 12:44:56,318 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 12:45:16,750 epoch 1 - iter 25/250 - loss 100.56233505 - samples/sec: 4.89 - lr: 0.050000\n",
            "2021-07-12 12:45:37,811 epoch 1 - iter 50/250 - loss 76.57731926 - samples/sec: 4.75 - lr: 0.050000\n",
            "2021-07-12 12:45:57,022 epoch 1 - iter 75/250 - loss 66.40456716 - samples/sec: 5.21 - lr: 0.050000\n",
            "2021-07-12 12:46:16,631 epoch 1 - iter 100/250 - loss 60.30278496 - samples/sec: 5.10 - lr: 0.050000\n",
            "2021-07-12 12:46:37,336 epoch 1 - iter 125/250 - loss 55.85715569 - samples/sec: 4.83 - lr: 0.050000\n",
            "2021-07-12 12:46:57,545 epoch 1 - iter 150/250 - loss 52.39555703 - samples/sec: 4.95 - lr: 0.050000\n",
            "2021-07-12 12:47:17,507 epoch 1 - iter 175/250 - loss 49.87136897 - samples/sec: 5.01 - lr: 0.050000\n",
            "2021-07-12 12:47:39,563 epoch 1 - iter 200/250 - loss 47.73133881 - samples/sec: 4.53 - lr: 0.050000\n",
            "2021-07-12 12:48:00,856 epoch 1 - iter 225/250 - loss 46.15333901 - samples/sec: 4.70 - lr: 0.050000\n",
            "2021-07-12 12:48:20,287 epoch 1 - iter 250/250 - loss 44.69551478 - samples/sec: 5.15 - lr: 0.050000\n",
            "2021-07-12 12:48:20,290 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 12:48:20,297 EPOCH 1 done: loss 44.6955 - lr 0.0500000\n",
            "2021-07-12 12:49:11,947 DEV : loss 27.788955688476562 - score 0.4923\n",
            "2021-07-12 12:49:12,098 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-07-12 12:49:14,428 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 12:49:31,890 epoch 2 - iter 25/250 - loss 28.05744469 - samples/sec: 5.73 - lr: 0.050000\n",
            "2021-07-12 12:49:50,450 epoch 2 - iter 50/250 - loss 29.72876740 - samples/sec: 5.39 - lr: 0.050000\n",
            "2021-07-12 12:50:07,741 epoch 2 - iter 75/250 - loss 29.82967560 - samples/sec: 5.78 - lr: 0.050000\n",
            "2021-07-12 12:50:24,131 epoch 2 - iter 100/250 - loss 29.52192453 - samples/sec: 6.10 - lr: 0.050000\n",
            "2021-07-12 12:50:42,995 epoch 2 - iter 125/250 - loss 29.93372169 - samples/sec: 5.30 - lr: 0.050000\n",
            "2021-07-12 12:50:59,133 epoch 2 - iter 150/250 - loss 29.85590443 - samples/sec: 6.20 - lr: 0.050000\n",
            "2021-07-12 12:51:15,313 epoch 2 - iter 175/250 - loss 29.39370006 - samples/sec: 6.18 - lr: 0.050000\n",
            "2021-07-12 12:51:32,145 epoch 2 - iter 200/250 - loss 29.37279575 - samples/sec: 5.94 - lr: 0.050000\n",
            "2021-07-12 12:51:49,205 epoch 2 - iter 225/250 - loss 29.40921231 - samples/sec: 5.86 - lr: 0.050000\n",
            "2021-07-12 12:52:06,041 epoch 2 - iter 250/250 - loss 29.12263071 - samples/sec: 5.94 - lr: 0.050000\n",
            "2021-07-12 12:52:06,044 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 12:52:06,046 EPOCH 2 done: loss 29.1226 - lr 0.0500000\n",
            "2021-07-12 12:52:31,036 DEV : loss 25.62730598449707 - score 0.5081\n",
            "2021-07-12 12:52:31,192 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-07-12 12:52:33,541 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 12:52:50,258 epoch 3 - iter 25/250 - loss 26.53978477 - samples/sec: 5.98 - lr: 0.050000\n",
            "2021-07-12 12:53:06,323 epoch 3 - iter 50/250 - loss 26.36312263 - samples/sec: 6.23 - lr: 0.050000\n",
            "2021-07-12 12:53:22,995 epoch 3 - iter 75/250 - loss 26.08265043 - samples/sec: 6.00 - lr: 0.050000\n",
            "2021-07-12 12:53:39,033 epoch 3 - iter 100/250 - loss 25.77597252 - samples/sec: 6.24 - lr: 0.050000\n",
            "2021-07-12 12:53:56,162 epoch 3 - iter 125/250 - loss 25.97670192 - samples/sec: 5.84 - lr: 0.050000\n",
            "2021-07-12 12:54:14,386 epoch 3 - iter 150/250 - loss 26.80690575 - samples/sec: 5.49 - lr: 0.050000\n",
            "2021-07-12 12:54:31,270 epoch 3 - iter 175/250 - loss 26.56233966 - samples/sec: 5.92 - lr: 0.050000\n",
            "2021-07-12 12:54:48,437 epoch 3 - iter 200/250 - loss 26.52958591 - samples/sec: 5.83 - lr: 0.050000\n",
            "2021-07-12 12:55:05,656 epoch 3 - iter 225/250 - loss 26.18186568 - samples/sec: 5.81 - lr: 0.050000\n",
            "2021-07-12 12:55:22,843 epoch 3 - iter 250/250 - loss 26.46386397 - samples/sec: 5.82 - lr: 0.050000\n",
            "2021-07-12 12:55:22,847 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 12:55:22,851 EPOCH 3 done: loss 26.4639 - lr 0.0500000\n",
            "2021-07-12 12:55:47,398 DEV : loss 20.892370223999023 - score 0.4024\n",
            "2021-07-12 12:55:47,549 BAD EPOCHS (no improvement): 1\n",
            "2021-07-12 12:55:47,550 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 12:56:02,643 epoch 4 - iter 25/250 - loss 23.28154305 - samples/sec: 6.63 - lr: 0.050000\n",
            "2021-07-12 12:56:18,854 epoch 4 - iter 50/250 - loss 24.03126587 - samples/sec: 6.17 - lr: 0.050000\n",
            "2021-07-12 12:56:36,985 epoch 4 - iter 75/250 - loss 25.10664291 - samples/sec: 5.52 - lr: 0.050000\n",
            "2021-07-12 12:56:53,954 epoch 4 - iter 100/250 - loss 24.95705475 - samples/sec: 5.89 - lr: 0.050000\n",
            "2021-07-12 12:57:11,595 epoch 4 - iter 125/250 - loss 24.74119893 - samples/sec: 5.67 - lr: 0.050000\n",
            "2021-07-12 12:57:28,822 epoch 4 - iter 150/250 - loss 25.35073318 - samples/sec: 5.81 - lr: 0.050000\n",
            "2021-07-12 12:57:46,824 epoch 4 - iter 175/250 - loss 25.27970691 - samples/sec: 5.56 - lr: 0.050000\n",
            "2021-07-12 12:58:03,908 epoch 4 - iter 200/250 - loss 24.98411219 - samples/sec: 5.85 - lr: 0.050000\n",
            "2021-07-12 12:58:21,486 epoch 4 - iter 225/250 - loss 24.81926327 - samples/sec: 5.69 - lr: 0.050000\n",
            "2021-07-12 12:58:38,588 epoch 4 - iter 250/250 - loss 25.02569069 - samples/sec: 5.85 - lr: 0.050000\n",
            "2021-07-12 12:58:38,592 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 12:58:38,596 EPOCH 4 done: loss 25.0257 - lr 0.0500000\n",
            "2021-07-12 12:59:03,489 DEV : loss 20.141666412353516 - score 0.5254\n",
            "2021-07-12 12:59:03,640 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-07-12 12:59:05,942 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 12:59:23,882 epoch 5 - iter 25/250 - loss 22.78090797 - samples/sec: 5.58 - lr: 0.050000\n",
            "2021-07-12 12:59:40,681 epoch 5 - iter 50/250 - loss 23.03526863 - samples/sec: 5.95 - lr: 0.050000\n",
            "2021-07-12 12:59:58,359 epoch 5 - iter 75/250 - loss 23.16296282 - samples/sec: 5.66 - lr: 0.050000\n",
            "2021-07-12 13:00:16,093 epoch 5 - iter 100/250 - loss 23.61811691 - samples/sec: 5.64 - lr: 0.050000\n",
            "2021-07-12 13:00:32,796 epoch 5 - iter 125/250 - loss 23.54363409 - samples/sec: 5.99 - lr: 0.050000\n",
            "2021-07-12 13:00:48,777 epoch 5 - iter 150/250 - loss 23.21050808 - samples/sec: 6.26 - lr: 0.050000\n",
            "2021-07-12 13:01:05,319 epoch 5 - iter 175/250 - loss 23.64592391 - samples/sec: 6.05 - lr: 0.050000\n",
            "2021-07-12 13:01:23,151 epoch 5 - iter 200/250 - loss 23.66953237 - samples/sec: 5.61 - lr: 0.050000\n",
            "2021-07-12 13:01:40,969 epoch 5 - iter 225/250 - loss 23.90168507 - samples/sec: 5.61 - lr: 0.050000\n",
            "2021-07-12 13:01:57,879 epoch 5 - iter 250/250 - loss 23.80791833 - samples/sec: 5.91 - lr: 0.050000\n",
            "2021-07-12 13:01:57,883 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 13:01:57,887 EPOCH 5 done: loss 23.8079 - lr 0.0500000\n",
            "2021-07-12 13:02:22,585 DEV : loss 19.654117584228516 - score 0.4906\n",
            "2021-07-12 13:02:22,734 BAD EPOCHS (no improvement): 1\n",
            "2021-07-12 13:02:22,736 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 13:02:40,000 epoch 6 - iter 25/250 - loss 23.76464775 - samples/sec: 5.80 - lr: 0.050000\n",
            "2021-07-12 13:02:58,018 epoch 6 - iter 50/250 - loss 24.25569576 - samples/sec: 5.55 - lr: 0.050000\n",
            "2021-07-12 13:03:15,635 epoch 6 - iter 75/250 - loss 23.98061925 - samples/sec: 5.68 - lr: 0.050000\n",
            "2021-07-12 13:03:32,594 epoch 6 - iter 100/250 - loss 23.65434834 - samples/sec: 5.90 - lr: 0.050000\n",
            "2021-07-12 13:03:49,648 epoch 6 - iter 125/250 - loss 23.19450531 - samples/sec: 5.86 - lr: 0.050000\n",
            "2021-07-12 13:04:07,328 epoch 6 - iter 150/250 - loss 23.56533704 - samples/sec: 5.66 - lr: 0.050000\n",
            "2021-07-12 13:04:24,066 epoch 6 - iter 175/250 - loss 23.30784124 - samples/sec: 5.97 - lr: 0.050000\n",
            "2021-07-12 13:04:41,374 epoch 6 - iter 200/250 - loss 23.31852407 - samples/sec: 5.78 - lr: 0.050000\n",
            "2021-07-12 13:04:57,938 epoch 6 - iter 225/250 - loss 23.26830593 - samples/sec: 6.04 - lr: 0.050000\n",
            "2021-07-12 13:05:14,475 epoch 6 - iter 250/250 - loss 23.17958958 - samples/sec: 6.05 - lr: 0.050000\n",
            "2021-07-12 13:05:14,482 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 13:05:14,484 EPOCH 6 done: loss 23.1796 - lr 0.0500000\n",
            "2021-07-12 13:05:40,774 DEV : loss 23.024606704711914 - score 0.5328\n",
            "2021-07-12 13:05:40,928 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-07-12 13:05:43,350 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 13:06:00,903 epoch 7 - iter 25/250 - loss 22.59022545 - samples/sec: 5.70 - lr: 0.050000\n",
            "2021-07-12 13:06:18,325 epoch 7 - iter 50/250 - loss 22.92812450 - samples/sec: 5.74 - lr: 0.050000\n",
            "2021-07-12 13:06:34,354 epoch 7 - iter 75/250 - loss 22.67747457 - samples/sec: 6.24 - lr: 0.050000\n",
            "2021-07-12 13:06:49,806 epoch 7 - iter 100/250 - loss 22.05160212 - samples/sec: 6.47 - lr: 0.050000\n",
            "2021-07-12 13:07:07,155 epoch 7 - iter 125/250 - loss 22.18537971 - samples/sec: 5.77 - lr: 0.050000\n",
            "2021-07-12 13:07:23,483 epoch 7 - iter 150/250 - loss 22.12369270 - samples/sec: 6.13 - lr: 0.050000\n",
            "2021-07-12 13:07:41,336 epoch 7 - iter 175/250 - loss 22.42680192 - samples/sec: 5.60 - lr: 0.050000\n",
            "2021-07-12 13:07:59,492 epoch 7 - iter 200/250 - loss 22.48597807 - samples/sec: 5.51 - lr: 0.050000\n",
            "2021-07-12 13:08:16,436 epoch 7 - iter 225/250 - loss 22.73135106 - samples/sec: 5.90 - lr: 0.050000\n",
            "2021-07-12 13:08:33,874 epoch 7 - iter 250/250 - loss 22.63767812 - samples/sec: 5.74 - lr: 0.050000\n",
            "2021-07-12 13:08:33,877 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 13:08:33,882 EPOCH 7 done: loss 22.6377 - lr 0.0500000\n",
            "2021-07-12 13:08:58,657 DEV : loss 19.336374282836914 - score 0.5147\n",
            "2021-07-12 13:08:58,804 BAD EPOCHS (no improvement): 1\n",
            "2021-07-12 13:08:58,806 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 13:09:15,012 epoch 8 - iter 25/250 - loss 20.52311295 - samples/sec: 6.17 - lr: 0.050000\n",
            "2021-07-12 13:09:31,631 epoch 8 - iter 50/250 - loss 21.07636127 - samples/sec: 6.02 - lr: 0.050000\n",
            "2021-07-12 13:09:49,886 epoch 8 - iter 75/250 - loss 22.04607315 - samples/sec: 5.48 - lr: 0.050000\n",
            "2021-07-12 13:10:07,204 epoch 8 - iter 100/250 - loss 22.01823122 - samples/sec: 5.78 - lr: 0.050000\n",
            "2021-07-12 13:10:24,167 epoch 8 - iter 125/250 - loss 22.04863348 - samples/sec: 5.90 - lr: 0.050000\n",
            "2021-07-12 13:10:40,250 epoch 8 - iter 150/250 - loss 21.73862689 - samples/sec: 6.22 - lr: 0.050000\n",
            "2021-07-12 13:10:56,974 epoch 8 - iter 175/250 - loss 21.83525586 - samples/sec: 5.98 - lr: 0.050000\n",
            "2021-07-12 13:11:14,324 epoch 8 - iter 200/250 - loss 22.21678089 - samples/sec: 5.76 - lr: 0.050000\n",
            "2021-07-12 13:11:29,956 epoch 8 - iter 225/250 - loss 21.89152439 - samples/sec: 6.40 - lr: 0.050000\n",
            "2021-07-12 13:11:47,785 epoch 8 - iter 250/250 - loss 22.21532069 - samples/sec: 5.61 - lr: 0.050000\n",
            "2021-07-12 13:11:47,792 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 13:11:47,794 EPOCH 8 done: loss 22.2153 - lr 0.0500000\n",
            "2021-07-12 13:12:12,847 DEV : loss 19.11396598815918 - score 0.5309\n",
            "2021-07-12 13:12:13,005 BAD EPOCHS (no improvement): 2\n",
            "2021-07-12 13:12:13,007 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 13:12:31,391 epoch 9 - iter 25/250 - loss 22.52010368 - samples/sec: 5.44 - lr: 0.050000\n",
            "2021-07-12 13:12:48,724 epoch 9 - iter 50/250 - loss 21.66997814 - samples/sec: 5.77 - lr: 0.050000\n",
            "2021-07-12 13:13:05,754 epoch 9 - iter 75/250 - loss 21.20179398 - samples/sec: 5.87 - lr: 0.050000\n",
            "2021-07-12 13:13:22,950 epoch 9 - iter 100/250 - loss 22.04519283 - samples/sec: 5.82 - lr: 0.050000\n",
            "2021-07-12 13:13:39,894 epoch 9 - iter 125/250 - loss 22.09185118 - samples/sec: 5.90 - lr: 0.050000\n",
            "2021-07-12 13:13:56,566 epoch 9 - iter 150/250 - loss 22.27390114 - samples/sec: 6.00 - lr: 0.050000\n",
            "2021-07-12 13:14:13,841 epoch 9 - iter 175/250 - loss 22.09950629 - samples/sec: 5.79 - lr: 0.050000\n",
            "2021-07-12 13:14:29,362 epoch 9 - iter 200/250 - loss 21.66559451 - samples/sec: 6.44 - lr: 0.050000\n",
            "2021-07-12 13:14:45,950 epoch 9 - iter 225/250 - loss 21.83829327 - samples/sec: 6.03 - lr: 0.050000\n",
            "2021-07-12 13:15:03,114 epoch 9 - iter 250/250 - loss 21.78676718 - samples/sec: 5.83 - lr: 0.050000\n",
            "2021-07-12 13:15:03,118 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 13:15:03,122 EPOCH 9 done: loss 21.7868 - lr 0.0500000\n",
            "2021-07-12 13:15:27,687 DEV : loss 18.638259887695312 - score 0.5291\n",
            "2021-07-12 13:15:27,841 BAD EPOCHS (no improvement): 3\n",
            "2021-07-12 13:15:27,843 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 13:15:44,708 epoch 10 - iter 25/250 - loss 20.85446411 - samples/sec: 5.93 - lr: 0.050000\n",
            "2021-07-12 13:16:02,558 epoch 10 - iter 50/250 - loss 22.23788971 - samples/sec: 5.60 - lr: 0.050000\n",
            "2021-07-12 13:16:19,155 epoch 10 - iter 75/250 - loss 21.61808594 - samples/sec: 6.03 - lr: 0.050000\n",
            "2021-07-12 13:16:36,822 epoch 10 - iter 100/250 - loss 21.30462589 - samples/sec: 5.66 - lr: 0.050000\n",
            "2021-07-12 13:16:54,621 epoch 10 - iter 125/250 - loss 21.39206526 - samples/sec: 5.62 - lr: 0.050000\n",
            "2021-07-12 13:17:11,971 epoch 10 - iter 150/250 - loss 21.42888200 - samples/sec: 5.76 - lr: 0.050000\n",
            "2021-07-12 13:17:29,879 epoch 10 - iter 175/250 - loss 21.41285704 - samples/sec: 5.58 - lr: 0.050000\n",
            "2021-07-12 13:17:47,017 epoch 10 - iter 200/250 - loss 21.10931776 - samples/sec: 5.84 - lr: 0.050000\n",
            "2021-07-12 13:18:03,463 epoch 10 - iter 225/250 - loss 21.04579242 - samples/sec: 6.08 - lr: 0.050000\n",
            "2021-07-12 13:18:19,790 epoch 10 - iter 250/250 - loss 21.28159544 - samples/sec: 6.13 - lr: 0.050000\n",
            "2021-07-12 13:18:19,793 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 13:18:19,794 EPOCH 10 done: loss 21.2816 - lr 0.0500000\n",
            "2021-07-12 13:18:44,690 DEV : loss 19.737668991088867 - score 0.5498\n",
            "2021-07-12 13:18:44,840 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-07-12 13:18:49,256 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 13:18:49,258 Testing using best model ...\n",
            "2021-07-12 13:18:49,267 loading file result/Bert__dataset_bs_4_lr_0.05_af_0.5_p_4_hsize_128_crf_1_lrnn_3_dp_0.3_wdp_0.05_ldp_0.5/best-model.pt\n",
            "2021-07-12 13:19:36,265 0.5432\t0.6538\t0.5934\n",
            "2021-07-12 13:19:36,268 \n",
            "Results:\n",
            "- F1-score (micro) 0.5934\n",
            "- F1-score (macro) 0.5934\n",
            "\n",
            "By class:\n",
            "KEY        tp: 3167 - fp: 2663 - fn: 1677 - precision: 0.5432 - recall: 0.6538 - f1-score: 0.5934\n",
            "2021-07-12 13:19:36,271 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX86W8zcyA1I"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEfKO7ljOrpm"
      },
      "source": [
        "# save train and dev predictions\n",
        "dev_DL = DataLoader(trainer.corpus.dev, batch_size=hyperparam.batch_size, num_workers=hyperparam.threads,)\n",
        "dev_eval_result, dev_loss = trainer.model.evaluate(trainer.corpus.dev,out_path=output+\"dev.tsv\")\n",
        "\n",
        "train_DL = DataLoader(trainer.corpus.train,batch_size=hyperparam.batch_size,num_workers=hyperparam.threads,)\n",
        "train_eval_result, train_loss = trainer.model.evaluate(trainer.corpus.train,out_path=output+\"train.tsv\")"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQJgtWZ0uyvL",
        "outputId": "2a623a5a-f81c-41a8-93f2-605d0cfd867d"
      },
      "source": [
        "# Print the results of the best model\n",
        "print(dev_eval_result.detailed_results)\n",
        "print(dev_loss)\n",
        "print(train_eval_result.detailed_results)\n",
        "print(train_loss)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Results:\n",
            "- F1-score (micro) 0.5498\n",
            "- F1-score (macro) 0.5498\n",
            "\n",
            "By class:\n",
            "KEY        tp: 2842 - fp: 2930 - fn: 1724 - precision: 0.4924 - recall: 0.6224 - f1-score: 0.5498\n",
            "tensor(19.6951, device='cuda:0')\n",
            "\n",
            "Results:\n",
            "- F1-score (micro) 0.5885\n",
            "- F1-score (macro) 0.5885\n",
            "\n",
            "By class:\n",
            "KEY        tp: 6546 - fp: 5923 - fn: 3232 - precision: 0.5250 - recall: 0.6695 - f1-score: 0.5885\n",
            "tensor(19.8058, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3mkY8GEoskP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        },
        "outputId": "aa39949f-3af1-4124-871a-0aeaeb300e91"
      },
      "source": [
        "# Plot the loss of the training and dev/validation set and F1 score evolution\n",
        "plotter = Plotter()\n",
        "plotter.plot_training_curves(output+'loss.tsv')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-12 13:20:05,310 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 13:20:05,312 WARNING: No LOSS found for test split in this data.\n",
            "2021-07-12 13:20:05,314 Are you sure you want to plot LOSS and not another value?\n",
            "2021-07-12 13:20:05,318 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 13:20:05,351 ----------------------------------------------------------------------------------------------------\n",
            "2021-07-12 13:20:05,352 WARNING: No F1 found for test split in this data.\n",
            "2021-07-12 13:20:05,356 Are you sure you want to plot F1 and not another value?\n",
            "2021-07-12 13:20:05,359 ----------------------------------------------------------------------------------------------------\n",
            "Loss and F1 plots are saved in result/Bert__dataset_bs_4_lr_0.05_af_0.5_p_4_hsize_128_crf_1_lrnn_3_dp_0.3_wdp_0.05_ldp_0.5/training.png\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMheoEMqEEtY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77973b66-4950-4f37-f908-bf8e2d144f06"
      },
      "source": [
        "# load the model and predcit the tags of a given sentence\n",
        "model = SequenceTagger.load(output+'best-model.pt')\n",
        "sentence = Sentence(\"Automatic terminology extraction or automatic keyphrase extraction is a very useful subfield of natural language processing when it comes to synthesizing information from texts in concise terms. In this master's thesis, this problem is approached with a sequence labeling approach using supervised deep learning techniques in specific domains, using bidirectional LTSM (Long short-term memory) neural networks and contextual word embeddings. A statistical significance study has been carried out to verify that the results presented in this work are significant. The final result in the F1 score in the Inspec dataset is 0.5730 slightly better than the higher result of the state of the art and it offers less dispersed results. Additionally, in this work the variation of samples in the training set is analyzed, a program to convert the datasets to a sequence labeling format needed for the final system is provided and there is an available sample program to test the keyphrase extractor in texts given by the user.\")\n",
        "model.predict(sentence)\n",
        "\n",
        "print(sentence.to_tagged_string())"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-12 13:20:07,657 loading file result/Bert__dataset_bs_4_lr_0.05_af_0.5_p_4_hsize_128_crf_1_lrnn_3_dp_0.3_wdp_0.05_ldp_0.5/best-model.pt\n",
            "Automatic <B-KEY> terminology <I-KEY> extraction <I-KEY> or automatic <B-KEY> keyphrase <I-KEY> extraction <I-KEY> is a very useful subfield of natural <B-KEY> language <I-KEY> processing <I-KEY> when it comes to synthesizing information from texts in concise terms . In this master 's thesis , this problem is approached with a sequence <B-KEY> labeling <I-KEY> approach <I-KEY> using supervised <B-KEY> deep <I-KEY> learning <I-KEY> techniques <I-KEY> in specific domains , using bidirectional <B-KEY> LTSM <I-KEY> ( Long <B-KEY> short-term <I-KEY> memory <I-KEY> ) neural <B-KEY> networks <I-KEY> and contextual <B-KEY> word <I-KEY> embeddings <I-KEY> . A statistical <B-KEY> significance <I-KEY> study <I-KEY> has been carried out to verify that the results presented in this work are significant . The final result in the F1 <B-KEY> score <I-KEY> in the Inspec <B-KEY> dataset <I-KEY> is 0.5730 slightly better than the higher result of the state of the art and it offers less dispersed results . Additionally , in this work the variation of samples in the training <B-KEY> set <I-KEY> is analyzed , a program to convert the datasets to a sequence <B-KEY> labeling <I-KEY> format <I-KEY> needed for the final system is provided and there is an available sample program to test the keyphrase <B-KEY> extractor <I-KEY> in texts given by the user .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76frstjNv7PS"
      },
      "source": [
        "# If the variable download_model is True, Colab will download the model (aprox 500 mb)\n",
        "if download_model:\n",
        "  files.download(output+'best-model.pt') "
      ],
      "execution_count": 52,
      "outputs": []
    }
  ]
}