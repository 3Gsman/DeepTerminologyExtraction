{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AKE_training_experiments.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eOuiHMXuoSe"
      },
      "source": [
        "# AUTOMATIC KEYWORD EXTRACTOR - Training experiments\n",
        "*Germán García García - gggsman@gmail.com*\n",
        "\n",
        "_________________________________\n",
        "\n",
        "MSC in Artificial Intelligence\n",
        "\n",
        "Final Master´s project: Applying Deep Learning Techniques to Terminology Extraction in Specific Domains\n",
        "\n",
        "Universidad Politecnica de Madrid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mBigCF53dHs"
      },
      "source": [
        "## Manual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfIv-KZUzy24"
      },
      "source": [
        "**Description:**\n",
        "\n",
        "The following Jupyter Notebook contains the cells of code needed to replicate the experiments presented in the document of the Final Master's Project, the main document can be found in the https://github.com/3Gsman/DeepTerminologyExtraction repository. It is an automatic keyword extractor system for training a model able to pick keywords from given texts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_lReIqM0eqy"
      },
      "source": [
        "**Abstract:**\n",
        "\n",
        "Automatic terminology extraction or automatic keyphrase extraction is a very useful subfield of natural language processing when it comes to synthesizing information from texts in concise terms. In this master's thesis, this problem is approached with a sequence labeling approach using supervised deep learning techniques in specific domains, using bidirectional LTSM (Long short-term memory) neural networks and contextual word embeddings. A statistical significance study has been carried out to verify that the results presented in this work are significant. The final result in the F1 score in the Inspec dataset is 0.5730 slightly better than the higher result of the state of the art and it offers less dispersed results. Additionally, in this work the variation of samples in the training set is analyzed, a program to convert the datasets to a sequence labeling format needed for the final system is provided and there is an available sample program to test the keyphrase extractor in texts given by the user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDJMBk8D0egd"
      },
      "source": [
        "**Usage:**\n",
        "\n",
        "To use this notebook you need to have a compatible dataset. Within this notebook, in the [github repository](https://github.com/3Gsman/DeepTerminologyExtraction) of this work, a folder named *datasets* is available, with all the datasets used during the experimentarion. Other option is to format your own dataset, in the same github repository can be found a script able to convert from 20 available keyword extraction datasets to the format needed in this notebook, please follow the instructions found in the folder *format_dataset*. The last option is to create or adapt your own dataset, the format needed is similar to CoNLL-03 but with only 3 tags: *B-KEY* for the word that begins the keyword, *I-KEY* for the subsequent words of the keyword, and *O* if that word it is not a keyword. If you have any question, plese contact with the author, Germán García in gggsman@gmail.com.\n",
        "\n",
        "Once you have the dataset downloaded, upload it to google colab, in the left part of the interface a folder shaped button should be clicked, click and drag the dataset to the *Files* section and wait until is uploaded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LjP21Aauoo6"
      },
      "source": [
        "## Editable variables and hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RM_IpkHv_Ov"
      },
      "source": [
        "# Set download_model to True if you want to download the model at the end of the execution\n",
        "download_model = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_BY6vtrxkEd"
      },
      "source": [
        "# Edit the hyperparameters if you want to change the traning behaviour\n",
        "class hyperparam:\n",
        "  embedding = 'Bert'\n",
        "  embedding_path = ''\n",
        "  dataset_base_path = ''\n",
        "  dataset = 'dataset'\n",
        "  output_base_path = 'result/'\n",
        "  iteration = ''\n",
        "  gpu = 1\n",
        "  lr = 0.05\n",
        "  anneal_factor = 0.5\n",
        "  patience = 4\n",
        "  batch_size = 4\n",
        "  num_epochs = 150\n",
        "  threads = 12\n",
        "  param_selection_mode = False\n",
        "  use_tensorboard = False\n",
        "  no_dev = False\n",
        "  use_crf = True\n",
        "  rnn_layers = 3\n",
        "  hidden_size = 128\n",
        "  dropout = 0.3\n",
        "  word_dropout = 0.05\n",
        "  locked_dropout = 0.5\n",
        "  not_in_memory = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jarXR_dAx4NQ"
      },
      "source": [
        "## System startup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBXenH5L4kLx"
      },
      "source": [
        "This section unzips the dataset and download the needed libraries to make this notebook work, also it imports the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUjzmKParSgA"
      },
      "source": [
        "!unzip dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6B6diatIetE"
      },
      "source": [
        "!pip install flair\n",
        "!pip install pytorch_transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6bv6_26uj6r"
      },
      "source": [
        "import sys\n",
        "from typing import List\n",
        "import argparse\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, FlairEmbeddings, CharacterEmbeddings, BertEmbeddings, TransformerXLEmbeddings, ELMoTransformerEmbeddings, ELMoEmbeddings,OpenAIGPTEmbeddings, RoBERTaEmbeddings,XLMEmbeddings, XLNetEmbeddings, OpenAIGPT2Embeddings\n",
        "from flair.datasets import DataLoader\n",
        "from flair.data import Corpus, Sentence\n",
        "from flair.datasets import ColumnCorpus\n",
        "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
        "import flair.datasets\n",
        "from flair.visual.training_curves import Plotter\n",
        "from torch.utils.data import DataLoader\n",
        "from flair.models import SequenceTagger\n",
        "from google.colab import files\n",
        "from flair.trainers import ModelTrainer\n",
        "import torch.optim as optim\n",
        "from pytorch_transformers import BertTokenizer\n",
        "from flair.models import SequenceTagger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7JHfKV0x734"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEVGHJtUx8eo"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyjIl55QwWaZ"
      },
      "source": [
        "def bs(tokenizer,x,l,r,max_seq_len):\n",
        "    if r>=l:\n",
        "        mid = int(l + (r - l)/2)\n",
        "        res=verifymid(tokenizer,x,mid,max_seq_len)\n",
        "        if res==3:\n",
        "            return mid\n",
        "        elif res==2:\n",
        "            return bs(tokenizer,x,mid+1,r,max_seq_len)\n",
        "        else:\n",
        "            return bs(tokenizer,x,l,mid-1,max_seq_len)\n",
        "            \n",
        "    else:\n",
        "        print(\"wrong binary search\")\n",
        "        sys.exit()\n",
        "\n",
        "\n",
        "def verifymid(tokenizer,x,mid,max_seq_len):\n",
        "    limit=mid\n",
        "    lw=x.to_tokenized_string().split(\" \")\n",
        "    lw=lw[:limit]\n",
        "    sent=\" \".join(lw)\n",
        "    tokenized_text = tokenizer.tokenize(sent)\n",
        "    if len(tokenized_text)>max_seq_len:\n",
        "        return 1\n",
        "    else:\n",
        "        if verifymid_1(tokenizer,x,mid+1,max_seq_len)==True:\n",
        "            return 2\n",
        "        return 3\n",
        "        \n",
        "        \n",
        "def verifymid_1(tokenizer,x,mid,max_seq_len):\n",
        "    limit=mid\n",
        "    lw=x.to_tokenized_string().split(\" \")\n",
        "    lw=lw[:limit]\n",
        "    sent=\" \".join(lw)\n",
        "    tokenized_text = tokenizer.tokenize(sent)\n",
        "    if len(tokenized_text)>max_seq_len:\n",
        "        return False\n",
        "    else:\n",
        "        return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bJHMtlcIbEP"
      },
      "source": [
        "def train(data_path, list_embedding, output, hyperparameter ):\n",
        "\n",
        "    # define columns\n",
        "    columns = {0: 'text', 1: 'ner'}\n",
        "\n",
        "    if hyperparam.no_dev==True:\n",
        "        corpus: Corpus = ColumnCorpus(data_path, \n",
        "                                      columns, \n",
        "                                      train_file='train.txt',\n",
        "                                      test_file='test.txt',\n",
        "                                      in_memory=not hyperparam.not_in_memory\n",
        "                                     )\n",
        "        \n",
        "    else:\n",
        "        corpus: Corpus = ColumnCorpus(data_path,\n",
        "                                      columns,\n",
        "                                      train_file='train.txt',\n",
        "                                      test_file='test.txt',\n",
        "                                      dev_file='dev.txt',\n",
        "                                      in_memory=not hyperparam.not_in_memory\n",
        "                                      )\n",
        "\n",
        "\n",
        "    # 2. what tag do we want to predict?\n",
        "    tag_type = 'ner'\n",
        "\n",
        "\n",
        "    # 3. make the tag dictionary from the corpus\n",
        "    tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
        "    print(tag_dictionary.idx2item)\n",
        "    \n",
        "    stats=corpus.obtain_statistics()\n",
        "    print(\"Original\\n\",stats)\n",
        "    \n",
        "\n",
        "    if hyperparam.embedding==\"Bert\":\n",
        "\n",
        "        print(\"Tokenizer\",hyperparam.embedding)\n",
        "        if hyperparam.embedding_path!=\"\":\n",
        "            tokenizer = BertTokenizer.from_pretrained(hyperparam.embedding_path)\n",
        "        else:\n",
        "            tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "        \n",
        "        max_seq_len=500   \n",
        "        print(\"taking max seq len as \",max_seq_len)\n",
        "           \n",
        "\n",
        "        new_train=[]\n",
        "        for x in corpus.train:\n",
        "\n",
        "            tokenized_text = tokenizer.tokenize(x.to_plain_string())\n",
        "\n",
        "            if len(tokenized_text)<=max_seq_len:\n",
        "                new_train.append(x)\n",
        "            \n",
        "            else:          \n",
        "                limit=bs(tokenizer,x,1,max_seq_len,max_seq_len)\n",
        "                lw=x.to_tokenized_string().split(\" \")\n",
        "                lw=lw[:limit]\n",
        "                sent=\" \".join(lw)\n",
        "                tokenized_text = tokenizer.tokenize(sent)\n",
        "                \n",
        "                if len(tokenized_text)>max_seq_len:\n",
        "                    print(\"wrong binary search 1\")\n",
        "                    sys.exit()\n",
        "\n",
        "                new_sent=Sentence(sent)\n",
        "                for index in range(len(new_sent)):\n",
        "                    try:\n",
        "                        new_sent[index].add_tag('ner', x[index].get_tag('ner').value)\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                new_train.append(new_sent)\n",
        "\n",
        "        new_test=[]\n",
        "        for x in corpus.test:\n",
        "\n",
        "            tokenized_text = tokenizer.tokenize(x.to_plain_string())\n",
        "\n",
        "            if len(tokenized_text)<=max_seq_len:\n",
        "                new_test.append(x)\n",
        "            \n",
        "            else:           \n",
        "                limit=bs(tokenizer,x,1,max_seq_len,max_seq_len)\n",
        "                lw=x.to_tokenized_string().split(\" \")\n",
        "                lw=lw[:limit]\n",
        "                sent=\" \".join(lw)\n",
        "                tokenized_text = tokenizer.tokenize(sent)\n",
        "\n",
        "                if len(tokenized_text)>max_seq_len:\n",
        "                    print(\"wrong binary search 1\")\n",
        "                    sys.exit()\n",
        "\n",
        "                new_sent=Sentence(sent)\n",
        "                for index in range(len(new_sent)):\n",
        "                    try:\n",
        "                        new_sent[index].add_tag('ner', x[index].get_tag('ner').value)\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                new_test.append(new_sent)\n",
        "\n",
        "        new_dev=[]\n",
        "        for x in corpus.dev:\n",
        "\n",
        "            tokenized_text = tokenizer.tokenize(x.to_plain_string())\n",
        "\n",
        "            if len(tokenized_text)<=max_seq_len:\n",
        "                new_dev.append(x)\n",
        "            \n",
        "            else:           \n",
        "                limit=bs(tokenizer,x,1,max_seq_len,max_seq_len)\n",
        "                lw=x.to_tokenized_string().split(\" \")\n",
        "                lw=lw[:limit]\n",
        "                sent=\" \".join(lw)\n",
        "                tokenized_text = tokenizer.tokenize(sent)\n",
        "\n",
        "                if len(tokenized_text)>max_seq_len:\n",
        "                    print(\"wrong binary search 1\")\n",
        "                    sys.exit()\n",
        "\n",
        "                new_sent=Sentence(sent)\n",
        "                for index in range(len(new_sent)):\n",
        "                    try:\n",
        "                        new_sent[index].add_tag('ner', x[index].get_tag('ner').value)\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                new_dev.append(new_sent)   \n",
        "\n",
        "        corpus._train=new_train\n",
        "        corpus._test=new_test\n",
        "        corpus._dev=new_dev\n",
        "        stats=corpus.obtain_statistics()\n",
        "        print(\"Modified\",stats)  \n",
        "\n",
        "\n",
        "    elif hyperparam.embedding==\"RoBERTa\":\n",
        "\n",
        "        print(\"Tokenizer\",hyperparam.embedding)\n",
        "\n",
        "        from pytorch_transformers import BertTokenizer\n",
        "        print(\"Using Bert tokenizer bert-base-uncased\")\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        \n",
        "        max_seq_len=500   \n",
        "        print(\"taking max seq len as \",max_seq_len)\n",
        "           \n",
        "        new_train=[]\n",
        "        for x in corpus.train:\n",
        "\n",
        "            tokenized_text = tokenizer.tokenize(x.to_plain_string())\n",
        "\n",
        "            if len(tokenized_text)<=max_seq_len:\n",
        "                new_train.append(x)\n",
        "            \n",
        "            else:            \n",
        "                limit=bs(tokenizer,x,1,max_seq_len,max_seq_len)\n",
        "                lw=x.to_tokenized_string().split(\" \")\n",
        "                lw=lw[:limit]\n",
        "                sent=\" \".join(lw)\n",
        "                tokenized_text = tokenizer.tokenize(sent)\n",
        "\n",
        "                if len(tokenized_text)>max_seq_len:\n",
        "                    print(\"wrong binary search 1\")\n",
        "                    sys.exit()\n",
        "\n",
        "                new_sent=Sentence(sent)\n",
        "                for index in range(len(new_sent)):\n",
        "                    try:\n",
        "                        new_sent[index].add_tag('ner', x[index].get_tag('ner').value)\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                new_train.append(new_sent)\n",
        "\n",
        "        new_test=[]\n",
        "        for x in corpus.test:\n",
        "            tokenized_text = tokenizer.tokenize(x.to_plain_string())\n",
        "            if len(tokenized_text)<=max_seq_len:\n",
        "                new_test.append(x)\n",
        "            \n",
        "            else:            \n",
        "                limit=bs(tokenizer,x,1,max_seq_len,max_seq_len)\n",
        "                lw=x.to_tokenized_string().split(\" \")\n",
        "                lw=lw[:limit]\n",
        "                sent=\" \".join(lw)\n",
        "                tokenized_text = tokenizer.tokenize(sent)\n",
        "\n",
        "                if len(tokenized_text)>max_seq_len:\n",
        "                    print(\"wrong binary search 1\")\n",
        "                    sys.exit()\n",
        "\n",
        "                new_sent=Sentence(sent)\n",
        "                for index in range(len(new_sent)):\n",
        "                    try:\n",
        "                        new_sent[index].add_tag('ner', x[index].get_tag('ner').value)\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                new_test.append(new_sent)\n",
        "\n",
        "        new_dev=[]\n",
        "        for x in corpus.dev:\n",
        "\n",
        "            tokenized_text = tokenizer.tokenize(x.to_plain_string())\n",
        "            \n",
        "            if len(tokenized_text)<=max_seq_len:\n",
        "                new_dev.append(x)\n",
        "            \n",
        "            else:            \n",
        "                limit=bs(tokenizer,x,1,max_seq_len,max_seq_len)\n",
        "                lw=x.to_tokenized_string().split(\" \")\n",
        "                lw=lw[:limit]\n",
        "                sent=\" \".join(lw)\n",
        "                tokenized_text = tokenizer.tokenize(sent)\n",
        "\n",
        "                if len(tokenized_text)>max_seq_len:\n",
        "                    print(\"wrong binary search 1\")\n",
        "                    sys.exit()\n",
        "\n",
        "                new_sent=Sentence(sent)\n",
        "                for index in range(len(new_sent)):\n",
        "                    new_sent[index].add_tag('ner', x[index].get_tag('ner').value)\n",
        "\n",
        "                new_dev.append(new_sent)             \n",
        "        \n",
        "        corpus._train=new_train\n",
        "        corpus._test=new_test\n",
        "        corpus._dev=new_dev\n",
        "        stats=corpus.obtain_statistics()\n",
        "        print(\"Modified\",stats)  \n",
        "\n",
        "\n",
        "    # 4. initialize embeddings\n",
        "    embedding_types: List[TokenEmbeddings] = list_embedding\n",
        "\n",
        "    embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "    # 5. initialize sequence tagger\n",
        "\n",
        "    tagger: SequenceTagger = SequenceTagger(hidden_size=hyperparam.hidden_size,\n",
        "                                        embeddings=embeddings,\n",
        "                                        tag_dictionary=tag_dictionary,\n",
        "                                        tag_type=tag_type,\n",
        "                                        use_crf=hyperparam.use_crf,\n",
        "                                        rnn_layers=hyperparam.rnn_layers,\n",
        "                                        dropout=hyperparam.dropout, word_dropout=hyperparam.word_dropout, locked_dropout=hyperparam.locked_dropout\n",
        "                                        )\n",
        "\n",
        "    # 6. initialize trainer\n",
        "\n",
        "    trainer: ModelTrainer = ModelTrainer(tagger, \n",
        "                                        corpus,\n",
        "                                        use_tensorboard=hyperparam.use_tensorboard,\n",
        "                                        optimizer=optim.SGD\n",
        "                                        ) \n",
        "\n",
        "    # 7. start training\n",
        "    trainer.train(output,\n",
        "              learning_rate=hyperparam.lr,\n",
        "              mini_batch_size=hyperparam.batch_size,\n",
        "              anneal_factor=hyperparam.anneal_factor,\n",
        "              patience=hyperparam.patience,\n",
        "              max_epochs=hyperparam.num_epochs,\n",
        "              param_selection_mode=hyperparam.param_selection_mode,\n",
        "              num_workers=hyperparam.threads,\n",
        "              \n",
        "              )\n",
        "    return trainer\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4beFhlmIpwx"
      },
      "source": [
        "if hyperparam.embedding=='Bert':\n",
        "    if hyperparam.embedding_path!=\"\":\n",
        "        embedding=BertEmbeddings(hyperparam.embedding_path)\n",
        "    else:\n",
        "        embedding=BertEmbeddings()\n",
        "\n",
        "if hyperparam.embedding=='ELMo':\n",
        "    if hyperparam.embedding_path!=\"\":\n",
        "        embedding=ELMoEmbeddings(hyperparam.embedding_path)\n",
        "    else:\n",
        "        embedding=ELMoEmbeddings()\n",
        "\n",
        "if hyperparam.embedding=='RoBERTa':\n",
        "    if hyperparam.embedding_path!=\"\":\n",
        "        embedding=RoBERTaEmbeddings(hyperparam.embedding_path)\n",
        "    else:\n",
        "        embedding=RoBERTaEmbeddings()\n",
        "\n",
        "output=hyperparam.output_base_path+hyperparam.embedding+\"_\"+hyperparam.embedding_path +\"_\"+hyperparam.dataset+hyperparam.iteration+\"_bs_\"+str(hyperparam.batch_size)+ \"_lr_\"+str(hyperparam.lr)+ '_af_'+str(hyperparam.anneal_factor)+ '_p_'+ str(hyperparam.patience) +\\\n",
        "               \"_hsize_\"+str(hyperparam.hidden_size)+\"_crf_\"+str(int(hyperparam.use_crf))+\"_lrnn_\"+str(hyperparam.rnn_layers)+\"_dp_\"+str(hyperparam.dropout)+\"_wdp_\"+str(hyperparam.word_dropout)+\"_ldp_\"+str(hyperparam.locked_dropout)+\"/\"\n",
        "dataset_path=hyperparam.dataset_base_path+hyperparam.dataset+\"/\"\n",
        "\n",
        "print(output)\n",
        "print(dataset_path)\n",
        "\n",
        "print(\"\\nHyper-Parameters\\n\")\n",
        "arguments=vars(hyperparam)\n",
        "for i in arguments:\n",
        "    print('{0:25}  {1}'.format(i+\":\", str(arguments[i])))\n",
        "    # print(i+\" : \"+str(arguments[i]))\n",
        "\n",
        "trainer=train(dataset_path,[embedding],output,hyperparam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX86W8zcyA1I"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEfKO7ljOrpm"
      },
      "source": [
        "a = DataLoader(trainer.corpus.dev, batch_size=hyperparam.batch_size, num_workers=hyperparam.threads,)\n",
        "\n",
        "#now save both train and dev predictions\n",
        "dev_eval_result, dev_loss = trainer.model.evaluate(trainer.corpus.dev,out_path=output+\"dev.tsv\")\n",
        "\n",
        "\n",
        "b = DataLoader(trainer.corpus.train,batch_size=hyperparam.batch_size,num_workers=hyperparam.threads,)\n",
        "\n",
        "train_eval_result, train_loss = trainer.model.evaluate(trainer.corpus.train,out_path=output+\"train.tsv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQJgtWZ0uyvL"
      },
      "source": [
        "print(dev_eval_result.detailed_results)\n",
        "print(dev_loss)\n",
        "print(train_eval_result.detailed_results)\n",
        "print(train_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3mkY8GEoskP"
      },
      "source": [
        "plotter = Plotter()\n",
        "plotter.plot_training_curves(output+'loss.tsv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMheoEMqEEtY"
      },
      "source": [
        "# load the model you trained\n",
        "model = SequenceTagger.load(output+'final-model.pt')\n",
        "# create example sentence\n",
        "sentence = Sentence(\"Automatic terminology extraction or automatic keyphrase extraction is a very useful subfield of natural language processing when it comes to synthesizing information from texts in concise terms. In this master's thesis, this problem is approached with a sequence labeling approach using supervised deep learning techniques in specific domains, using bidirectional LTSM (Long short-term memory) neural networks and contextual word embeddings. A statistical significance study has been carried out to verify that the results presented in this work are significant. The final result in the F1 score in the Inspec dataset is 0.5730 slightly better than the higher result of the state of the art and it offers less dispersed results. Additionally, in this work the variation of samples in the training set is analyzed, a program to convert the datasets to a sequence labeling format needed for the final system is provided and there is an available sample program to test the keyphrase extractor in texts given by the user.\")\n",
        "# predict tags and print\n",
        "model.predict(sentence)\n",
        "\n",
        "print(sentence.to_tagged_string())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76frstjNv7PS"
      },
      "source": [
        "if download_model:\n",
        "  files.download(output+'best-model.pt') "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}